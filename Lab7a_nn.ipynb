{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab7a: Introduction to Tensorflow #\n",
    "\n",
    "In this lab, you will start working with [Tensorflow](http://tensorflow.org/), a cutting-edge library for developing, and evaluating deep neural network models.\n",
    "\n",
    "Tensorflow is a relatively new library (a few years old).  For this module, **assume all later labs and assignment will utilize Tensorflow Version 1.4**. \n",
    "\n",
    "First, start by reading and working through the Tensorflow MNIST Tutorial: https://www.tensorflow.org/get_started/mnist/beginners\n",
    "Follow their link to installation help for setting up Tensor Flow.\n",
    "[https://www.tensorflow.org/install/](https://www.tensorflow.org/install/). \n",
    "\n",
    "** After you have completed the bove MNIST for begginers tutorial**, complete the following three tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Import Tensorflow ###\n",
    "\n",
    "Run the following python code, it should return the tensor flow version you have installed with no errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow 101 - The Computation Graph ##\n",
    "\n",
    "Writing Tensorflow code is very different than writing regular Python code. This is because the bulk of the computation in Tensorflow is executed in a separate process, containing high performing C code. However, instead of directly writing everything in C, Tensorflow defines a Python API, for both building the C computations, and for reading and writing data from/to this separate process.\n",
    "\n",
    "Here are the things to keep in mind.\n",
    "\n",
    "The core of Tensorflow is the **Computation Graph**. Tensorflow starts and ends with this graph - every operation you define, every intermediate value you calculate lives here.\n",
    "    \n",
    "  + This graph consists of nodes and edges. Nodes are **Tensors**, or matrices of varying dimensions (i.e 3D, 4D, etc.). The edges are **Operations** that take one or more tensors, and produce a new, resulting Tensor after applying a given transformation (i.e. addition, subtraction, matrix multiplication, etc.)\n",
    "  + All of these Tensors and Operations exist in this separate, high performing process. That means you can't print/peek into the Tensors like you would in regular Python.\n",
    "  + This graph defines a \"flow\" of Tensors, starting from an input, resulting in a desired output. Hence the library's name, **Tensorflow**.\n",
    "  \n",
    "There are two steps to every Tensorflow program:\n",
    "\n",
    "  + Defining the graph: Use Python's Tensorflow API to set up the inputs, all the transformations, the operations to run, and the desired outputs to collect.\n",
    "  + Interacting with the graph: Feed inputs into the Computation Graph, and collect outputs, transforming normal Python objects into Tensors to be read by the Computation Graph.\n",
    "  \n",
    "We'll be talking about how to define the Computation Graph soon. However, the crucial thing to understand is this second point - how to feed data to the computation graph, and how to read outputs back.\n",
    "\n",
    "The device that allows us to communicate with the computation graph is the **Session.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Sessions - Portal to the Computation Graph ###\n",
    "\n",
    "All of your interaction with the Computation Graph will happen through a **Session** object. Sessions allow you to not only load Python objects into the Graph, but they also allow you to run arbitrary operations, and fetch the values of given Tensors.\n",
    "\n",
    "To better understand this, consider the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define a Constant Tensor on the Computation Graph\n",
    "hello = tf.constant('Hello World!')\n",
    "\n",
    "# Print the Tensor - this is not a normal Python object - it's a Tensor!!!\n",
    "print(hello)\n",
    "\n",
    "# Create a Session\n",
    "session = tf.Session()\n",
    "\n",
    "# Run (Fetch) the given Tensor, and print it's value - this is a normal Python object (str)\n",
    "print(session.run(hello))\n",
    "\n",
    "# Close the session\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sessions act a lot like files in Python. They need to be opened, via the special ```tf.Session()``` constructor, and assigned to a variable. Then, to read a Tensor value from a session, you need to call ```session.run(val)``` to evaluate the set of operations resulting in the given Tensor. You must then close the session, to end the interaction.\n",
    "\n",
    "You can also *feed* values into a Computation Graph, via **feed_dicts** (feed dictionaries). These dictionaries provide a mapping between special Tensor objects called placeholders, that denote inputs coming from normal Python, and actual raw python objects.\n",
    "\n",
    "An example is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a normal Python string\n",
    "hello = \"Hello World!\"\n",
    "\n",
    "# Define a Placeholder Tensor on the Computation Graph - note that you have to define the type of a placeholder!\n",
    "string_tensor = tf.placeholder(dtype=tf.string)\n",
    "\n",
    "# Print the String Tensor - See what it looks like!\n",
    "print(string_tensor)\n",
    "\n",
    "# Create a Session\n",
    "session = tf.Session()\n",
    "\n",
    "# Run (Fetch) the given placeholder, but after feeding in the value in `hello`\n",
    "print(session.run(string_tensor, feed_dict={string_tensor: hello}))\n",
    "\n",
    "# Close Session\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play around with the above example to make sure that everything makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task #2 Placeholders, Operations, and Sessions ###\n",
    "\n",
    "The following code block is incomplete. Using your knowledge of Placeholders, the Computation Graph, and Sessions, have the following code print \"Hello NAME!\" after reading your name from STDIN.\n",
    "\n",
    "Hint 1: You might want to look at the Tensorflow API/Stack Overflow for how to concatenate String Tensors in Tensor Flow.\n",
    "\n",
    "Hint 2: Check out `tf.add`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get your name from stdin\n",
    "name = input(\"Enter your name here: \")\n",
    "\n",
    "# Define a Placeholder Tensor for your name\n",
    "name_tensor = tf.placeholder(dtype=tf.string)\n",
    "\n",
    "# Define a Constant Tensor\n",
    "hello_tensor = tf.constant(\"Hello \")\n",
    "exclamation_tensor = tf.constant(\"!\")\n",
    "\n",
    "# OPERATIONS GO HERE\n",
    "hello_name_tensor = ?\n",
    "\n",
    "# Create a Session\n",
    "session = tf.Session()\n",
    "\n",
    "# RUN YOUR SESSION HERE, AND DISPLAY THE RESULTS\n",
    "hello_name = ?\n",
    "print(hello_name)\n",
    "\n",
    "# Close Session\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all Together - Placeholder Shapes, Variables, and Neural Networks ###\n",
    "\n",
    "There are two other big things to understand about Tensorflow. The first is related to Placeholders, like we've seen before. Whereas in the above examples, we only define our placeholders with a dtype (the type of input it will hold), placeholders usually are also defined by their *shape*, or matrix dimensions (like in Numpy). \n",
    "\n",
    "Consider the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist_placeholder = tf.placeholder(dtype=tf.float32, shape=[784])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we define a placeholder of type float32, with a shape (dimension) of 784. In other words, our mnist_placeholder stores float vectors with 784 elements (hmm, seems awfully familiar)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other big thing in Tensorflow are **Variables.** Like we've seen in class, neural networks are defined by a set of parameters, that we learn during the training process. These parameters are **Variables** that are special types of Tensors that are slightly different than the placeholders, or the constants we've looked at before. \n",
    "\n",
    "Furthermore, Variables are defined with a special syntax, and must be initialized (via a call to session.run) first, before any other evaluation.\n",
    "\n",
    "Here is an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Variable for MNIST Classifier Weight - initialize Variable to be zero matrix, of shape [784, 10]\n",
    "W = tf.Variable(initial_value=tf.zeros(shape=[784, 10]))\n",
    "print(W)\n",
    "\n",
    "# Create Variable for MNIST Classifier Bias - initialize Variable ot be zero vector with 10 elements\n",
    "b = tf.Variable(initial_value=tf.zeros(shape=[10]))\n",
    "print(b)\n",
    "\n",
    "# Create Session\n",
    "session = tf.Session()\n",
    "\n",
    "# Initialize all Variables => Special call => REMEMBER THIS!\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "# Close Session\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 - MNIST Classifier in Tensorflow ###\n",
    "\n",
    "We now have all the pieces we need to start using Tensorflow effectively.  Below is a partially implemented MNISt Classifier. \n",
    "\n",
    "It has a lot of information we've already covered, and a lot of the code you'll need to become familiar with to start writing Neural Network Models in Tensorflow, including activation functions, matrix multiplication, and training via SGD. Note that in Tensorflow, all gradient calculations are taken care of for you.\n",
    "\n",
    "Complete following code according to the provided comments, and try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "\n",
    "# Read MNIST Dataset from TF Helper\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# Setup Parameters\n",
    "input_size, num_classes = 784, 10\n",
    "num_train_steps = 10000\n",
    "learning_rate = 0.5\n",
    "\n",
    "# Create Placeholders, Variables\n",
    "x_tensor = ?     # Placeholder for Input Image => Single Vector, Dimension [1, 784]\n",
    "y_tensor = ?     # Placeholder for Output Class => Vector, One-Hot, Dimension [1, 10]\n",
    "W = ?            # Weights, use tf.random_normal with stddev = .1\n",
    "b = ?            # Bias, use tf.random_normal with stddev = .1\n",
    "\n",
    "# Use operations to generate final logits (no softmax)\n",
    "# logits = x*W + b\n",
    "logits = ?\n",
    "\n",
    "# Get probabilities by using the softmax activation on the given logits\n",
    "probabilities = tf.nn.softmax(logits)\n",
    "\n",
    "# Compute Loss Value via TF Loss Helper\n",
    "loss = tf.losses.softmax_cross_entropy(onehot_labels=y_tensor, logits=logits)\n",
    "\n",
    "# Create Gradient Descent Optimizer, training operation for updating weights\n",
    "sgd = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "train_op = sgd.minimize(loss)\n",
    "\n",
    "# Create Session\n",
    "session = tf.Session()\n",
    "\n",
    "# Initialize all Variables\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "# Training Loop!\n",
    "for i in range(num_train_steps):\n",
    "    # Get next element from the MNIST Training Data\n",
    "    next_x, next_y = mnist.train.next_batch(1)\n",
    "    \n",
    "    # Collect/Run Loss, Training Operation via single call to session.run (note multiple fetches!)\n",
    "    l, _ = session.run([loss, train_op], feed_dict={x_tensor: next_x, y_tensor: next_y})\n",
    "    \n",
    "    # Print Loss every so often\n",
    "    if i % 1000 == 0:\n",
    "        print('Iteration %d\\tLoss Value: %.3f' % (i, l))\n",
    "        \n",
    "# Evaluate Accuracy on Test Data\n",
    "correct, test_x, test_y = 0.0, mnist.test.images, mnist.test.labels\n",
    "for i in range(10000):\n",
    "    next_x, next_y = test_x[i], test_y[i]\n",
    "    p = session.run([probabilities], feed_dict={x_tensor: [next_x], y_tensor: [next_y]})\n",
    "    if np.argmax(p[0]) == np.argmax(next_y):\n",
    "        correct += 1\n",
    "print('Test Accuracy: %.3f' % (correct / 10000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
